{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **KSA Telecom's Customer Arabic Sentiment Analysis**"
      ],
      "metadata": {
        "id": "hoiMhRRxoDIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "The volume of Arabic posts on numerous social networks has increased dramatically, giving a rich supply of viewpoints on a wide range of topics. While opinion mining has sparked a lot of academic interest in the previous decade, it has primarily been done in English. Opinion mining from Arabic social media is behind, owing to difficulties in processing the complex Arabic natural language and a lack of tools and resources for labelling Arabic dataset and extracting Arabic feelings from the text. The goal of this project is to perform sentiment analysis on Arabic tweets using customer care tweets in the KSA. This data contains 10,000 tweets for a telecom company's customer care account on Twitter.\n",
        "\n",
        "---\n",
        "\n",
        "**Project Description**\n",
        "\n",
        "The project was divided into four main phases:\n",
        "  *   Preprocessing\n",
        "  *   Labeling\n",
        "  *   Feature Extraction\n",
        "  *   Classification\n",
        "\n",
        "---\n",
        "\n",
        "**Source of Data:**\n",
        "\n",
        "https://www.kaggle.com/datasets/mansourhussain/customer-care-tweets-ksa\n"
      ],
      "metadata": {
        "id": "7u5mZ657pB7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Python Libraries**"
      ],
      "metadata": {
        "id": "egnHG7iQxmU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Data Processing\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Data Labelling\n",
        "import httpx\n",
        "from googletrans import Translator\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Analysis and Visualisation\n",
        "from tabulate import tabulate\n",
        "import codecs\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Feature Extraction\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Clasiffication\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "vEQCGVXXcObq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Uploading**"
      ],
      "metadata": {
        "id": "q6FjzWm1yDZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/File_Path/stc_care_tweets.csv', encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "SOmdDrPkveJU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**"
      ],
      "metadata": {
        "id": "un4g6uYSuwLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process(tweet):\n",
        "    # Replace @username with empty string\n",
        "    tweet = re.sub('@[^\\s]+',' ', tweet)\n",
        "    # Replace www.* or https?://* with empty string\n",
        "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ', tweet)\n",
        "    # Replace #word with with empty string\n",
        "    tweet = re.sub('#([^\\s]+)',' ', tweet)\n",
        "    # Replace the numbers with empty string\n",
        "    tweet = re.sub('\\d+',' ', tweet)\n",
        "    # Replace punctuations with empty string\n",
        "    tweet = re.sub('[^\\w\\s]',' ', tweet)\n",
        "    # Replace english alphabet letter with empty string\n",
        "    tweet = re.sub('[a-zA-Z]',' ', tweet)\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "9ITdzEz4x0z3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying pre_process function for preprocessing\n",
        "df[\"clean_text\"] = df['Text'].apply(lambda x:pre_process(x))"
      ],
      "metadata": {
        "id": "YCF95NcayHVE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.applymap(lambda x:x.strip() if isinstance(x, str) else x)\n",
        "df['clean_text'].replace('', np.nan, inplace=True)\n",
        "df.dropna(subset=['clean_text'], inplace=True)"
      ],
      "metadata": {
        "id": "UFVuuGsUZSxC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace emoji with empty string\n",
        "df['clean_text'] = df['clean_text'].apply(lambda s:emoji.replace_emoji(s, ''))\n",
        "# Remove the daplicate raw\n",
        "df = df.drop_duplicates()\n",
        "# Tokenize the tweet text\n",
        "df['clean_text'] = df['clean_text'].apply(nltk.word_tokenize)\n",
        "# Remove the stop words\n",
        "stopwords_list = stopwords.words('arabic')\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x:[item for item in x if item not in stopwords_list])\n",
        "# lemmatize the tweet text\n",
        "lemmatizer  = WordNetLemmatizer()\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x:[lemmatizer.lemmatize(word) for word in x])"
      ],
      "metadata": {
        "id": "0498nbQ-ihnH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Labilling**"
      ],
      "metadata": {
        "id": "gu-3cpaXlnB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timeout = httpx.Timeout(5) # 5 seconds timeout\n",
        "translator = Translator(timeout=timeout)\n",
        "df['clean_text'] = df['clean_text'].astype(str)\n",
        "df['English'] = df['clean_text'].apply(translator.translate, src='ar', dest='en').apply(getattr, args=('text',))"
      ],
      "metadata": {
        "id": "Hk-yvmePDLXR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TextBlob Data Lablling\n",
        "from textblob import TextBlob\n",
        "df['Sentiment'] = ''\n",
        "for i,x in df.English.iteritems():\n",
        "  label = TextBlob(x)\n",
        "  df['Sentiment'][i] = label.sentiment.polarity\n",
        "def polarity_to_label(x):\n",
        "    if(x >= -1 and x < 0):\n",
        "        return 'Negative'\n",
        "    if(x == 0):\n",
        "        return 'Neutral'\n",
        "    if(x > 0 and x <= 1):\n",
        "        return 'Positive'\n",
        "df.Sentiment = df.Sentiment.apply(polarity_to_label)"
      ],
      "metadata": {
        "id": "qCxbXFTGDMgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analysis and Visualisation**"
      ],
      "metadata": {
        "id": "twwM6zeWNM0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total number of tweets\n",
        "Tweets_Count = len(df)\n",
        "# Count number of unique users\n",
        "Unique_Users = len(df['Tweet Id'].unique())\n",
        "dict_Tweets  = {'Name':['Tweets_Count', 'User_Count'], 'Count':[Tweets_Count, Unique_Users]}\n",
        "head = [\"Name\", \"Count\"]\n",
        "print(tabulate(dict_Tweets, headers=head, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "id": "NAxTegmDNap3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "-rVztjTG3r4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# People's feelings towards the Telecom companies\n",
        "matplotlib.style.use('tableau-colorblind10')\n",
        "df['Sentiment'].value_counts().plot.pie(autopct='%1.1f%%', wedgeprops={'linewidth':2.0, 'edgecolor': 'white'}, textprops={'fontsize':8})\n",
        "plt.title(\"Customer Feelings Towards KSA Telecom\", fontsize=10)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EG5mAMAbm4_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Features Extraction**"
      ],
      "metadata": {
        "id": "pVK8Lf1f6aj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF features extraction\n",
        "word_vectorizer = TfidfVectorizer(sublinear_tf=True,strip_accents='unicode', analyzer='word', ngram_range=(1, 1), max_features =1000)\n",
        "unigramdataGet= word_vectorizer.fit_transform(df['clean_text'].astype('str'))\n",
        "unigramdataGet = unigramdataGet.toarray()\n",
        "vocab = word_vectorizer.get_feature_names_out()\n",
        "features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)\n",
        "features[features>0] = 1\n",
        "features.head()\n",
        "pro = preprocessing.LabelEncoder()\n",
        "encpro = pro.fit_transform(df['Sentiment'])\n",
        "df['label'] = encpro\n",
        "y = df['Sentiment']\n",
        "X = features"
      ],
      "metadata": {
        "id": "lOqBUdYaN8zh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Classification**"
      ],
      "metadata": {
        "id": "jQjMantpKqGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting Dataset into 70% Training and 30% Testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=333)"
      ],
      "metadata": {
        "id": "yfeiHsFMLRzn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForest Algorithm\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "RF =clf.score(X_test, y_test)\n",
        "print('Accuracy= {:.3f}'.format(clf.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "pRuE0LgMSWZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "85cDB22Iwq1h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}